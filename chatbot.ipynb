{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chatbot - NLP and Deep Learning**\n",
    "\n",
    "For this project, we will use the PyTorch library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Theory and NLP concepts\n",
    "\n",
    "We will talk about stemming, tokenization, bag of words.\n",
    "\n",
    "First, we put all words (of each patterns) into an array.\n",
    "\n",
    "- **Bag of words :** For each different pattern, we create an array w/ the same size as the all words array. If this word is included into the all words array, we put a 1 at his position, 0 otherwise.\n",
    "- **Tokenization :** Splitting string into meaningful units (e.g. words, punctuation characters, numbers)\n",
    "- **Stemming :** Generate the root form of the words. It is an heuristic that chops of the ends off of words. \n",
    "\n",
    "### **Whole NLP pre-processing pipeline :** \n",
    "\n",
    "At the beginning, we have the Whole sentence, then we tokenize it. We lower all the words, then we stem the words. We then exclude punctuation characters. And based on this array, we calculate the bag of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Create training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a free Natural Language data, using a framework, called NLTK - Natural Language toolkit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bradfo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download a package from nltk\n",
    "nltk.download('punkt')   # package w/ a pre-trained tokenizer\n",
    "\n",
    "# Stemming: reduce a word to its root form\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Tokenize: split a sentence into a list of words\n",
    "def tokenize(sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "def stem(word):\n",
    "    return stemmer.stem(word.lower())\n",
    "\n",
    "def bag_of_words(tokenized_sentence, all_words):\n",
    "    \"\"\"\n",
    "    sentence = [\"hello\", \"how\", \"are\", \"you\"]\n",
    "    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
    "    bag =   [  0,      1,      0,    1,      0,      0,       0]\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = [stem(w) for w in tokenized_sentence]\n",
    "    \n",
    "    bag = np.zeros(len(all_words), dtype=np.float32)\n",
    "    for idx, w in enumerate(all_words):\n",
    "        if w in tokenized_sentence:\n",
    "            bag[idx] = 1.0\n",
    "            \n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('intents.json', 'r') as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "\n",
    "# Loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:   #key: intents, value: list of intents\n",
    "    tag = intent['tag']             #key: tag, value: intent\n",
    "    tags.append(tag)\n",
    "    # Loop through each pattern in the patterns\n",
    "    for pattern in intent['patterns']:\n",
    "        # Tokenize each word in the sentence\n",
    "        w = tokenize(pattern)\n",
    "        # Add to our words list (not append, because we don't want a list of lists)\n",
    "        all_words.extend(w)\n",
    "        # Add to xy pair\n",
    "        # pattern and tag for each pattern\n",
    "        xy.append((w, tag))\n",
    "\n",
    "# Stem and lower each word and remove duplicates\n",
    "ignore_words = ['?', '!', '.', ',']\n",
    "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
    "\n",
    "# Sort all words and remove duplicates\n",
    "all_words = sorted(set(all_words))\n",
    "# Sort tags and remove duplicates\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "# Create training data\n",
    "X_train = [] # bag of words for each pattern\n",
    "y_train = [] # label for each tag\n",
    "\n",
    "for (pattern_sentence, tag) in xy:\n",
    "    # X: bag of words for each pattern\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "    # y: PyTorch CrossEntropyLoss\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label) # CrossEntropyLoss\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    # Dataset[idx]\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    # len(Dataset)\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "hidden_size = 8\n",
    "output_size = len(tags)\n",
    "input_size = len(X_train[0])\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "\n",
    "dataset = ChatDataset()\n",
    "\n",
    "# Data loader which takes the dataset, shuffles it, and creates batches\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. PyTorch model and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Feed Forward Neural Net\n",
    "\n",
    "- Input_size : Number of different patterns which is fixed\n",
    "- Hidden_size : can be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__() # inherit from nn.Module\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        # Create an activation function for in-between layers\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    # Our model\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        # no activation and no softmax because we use cross entropy loss\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 1.4959\n",
      "Epoch [200/1000], Loss: 0.8032\n",
      "Epoch [300/1000], Loss: 0.4153\n",
      "Epoch [400/1000], Loss: 0.0310\n",
      "Epoch [500/1000], Loss: 0.2056\n",
      "Epoch [600/1000], Loss: 0.0913\n",
      "Epoch [700/1000], Loss: 0.0043\n",
      "Epoch [800/1000], Loss: 0.0081\n",
      "Epoch [900/1000], Loss: 0.0019\n",
      "Epoch [1000/1000], Loss: 0.0010\n",
      "final loss: 0.0010\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #verify if GPU is available\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device) #push it to device if it's available\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(words)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward and optimizer step\n",
    "        optimizer.zero_grad() #empty the gradients first\n",
    "        loss.backward() #calculate the gradients / the backpropagation\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(f'final loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Save and load model and implement the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete. file savec to data.pth\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"input_size\": input_size,\n",
    "    \"output_size\": output_size,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"all_words\": all_words,\n",
    "    \"tags\": tags\n",
    "}\n",
    "\n",
    "# Serialize it\n",
    "FILE = \"data.pth\" # for pytorch\n",
    "\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f'training complete. file savec to {FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Implement the chat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's chat! type 'quit' to exit\n",
      "Sam: Hello!\n",
      "Sam: Hello!\n",
      "Sam: Hello!\n",
      "Sam: Good to see you again!\n",
      "Sam: Hello!\n",
      "Sam: Hello!\n",
      "Sam: Hi there, how can I help?\n",
      "Sam: Hello!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data[\"all_words\"]\n",
    "tags = data[\"tags\"]\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval() # set model to evaluation mode\n",
    "\n",
    "bot_name = \"Sam\"\n",
    "print(\"Let's chat! type 'quit' to exit\")\n",
    "\n",
    "while True:\n",
    "    sentence = input('You: ')\n",
    "    if sentence == \"quit\" :\n",
    "        break\n",
    "    sentence = tokenize(sentence)\n",
    "    X = bag_of_words(sentence, all_words)\n",
    "    X = X.reshape(1, X.shape[0]) # reshape to fit the model (one sample)\n",
    "    X = torch.from_numpy(X)\n",
    "\n",
    "    output = model(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "    tag = tags[predicted.item()] #predicted.item : class label\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "\n",
    "    if prob.item() > 0.75:\n",
    "        for intent in intents[\"intents\"]:\n",
    "            if tag == intent[\"tag\"]:\n",
    "                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "    else:\n",
    "        print(f\"{bot_name}: I do not understand...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
