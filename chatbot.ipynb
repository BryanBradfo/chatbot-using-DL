{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chatbot - NLP and Deep Learning**\n",
    "\n",
    "For this project, we will use the PyTorch library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Theory and NLP concepts\n",
    "\n",
    "We will talk about stemming, tokenization, bag of words.\n",
    "\n",
    "First, we put all words (of each patterns) into an array.\n",
    "\n",
    "- **Bag of words :** For each different pattern, we create an array w/ the same size as the all words array. If this word is included into the all words array, we put a 1 at his position, 0 otherwise.\n",
    "- **Tokenization :** Splitting string into meaningful units (e.g. words, punctuation characters, numbers)\n",
    "- **Stemming :** Generate the root form of the words. It is an heuristic that chops of the ends off of words. \n",
    "\n",
    "### **Whole NLP pre-processing pipeline :** \n",
    "\n",
    "At the beginning, we have the Whole sentence, then we tokenize it. We lower all the words, then we stem the words. We then exclude punctuation characters. And based on this array, we calculate the bag of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Create training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a free Natural Language data, using a framework, called NLTK - Natural Language toolkit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download a package from nltk\n",
    "nltk.download('punkt')   # package w/ a pre-trained tokenizer\n",
    "\n",
    "# Stemming: reduce a word to its root form\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Tokenize: split a sentence into a list of words\n",
    "def tokenize(sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "def stem(word):\n",
    "    return stemmer.stem(word.lower())\n",
    "\n",
    "def bag_of_words(tokenized_sentence, all_words):\n",
    "    \"\"\"\n",
    "    sentence = [\"hello\", \"how\", \"are\", \"you\"]\n",
    "    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
    "    bag =   [  0,      1,      0,    1,      0,      0,       0]\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = [stem(w) for w in tokenized_sentence]\n",
    "    \n",
    "    bag = np.zeros(len(all_words), dtype=np.float32)\n",
    "    for idx, w in enumerate(all_words):\n",
    "        if w in tokenized_sentence:\n",
    "            bag[idx] = 1.0\n",
    "            \n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('intents.json', 'r') as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "\n",
    "# Loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:   #key: intents, value: list of intents\n",
    "    tag = intent['tag']             #key: tag, value: intent\n",
    "    tags.append(tag)\n",
    "    # Loop through each pattern in the patterns\n",
    "    for pattern in intent['patterns']:\n",
    "        # Tokenize each word in the sentence\n",
    "        w = tokenize(pattern)\n",
    "        # Add to our words list (not append, because we don't want a list of lists)\n",
    "        all_words.extend(w)\n",
    "        # Add to xy pair\n",
    "        # pattern and tag for each pattern\n",
    "        xy.append((w, tag))\n",
    "\n",
    "# Stem and lower each word and remove duplicates\n",
    "ignore_words = ['?', '!', '.', ',']\n",
    "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
    "\n",
    "# Sort all words and remove duplicates\n",
    "all_words = sorted(set(all_words))\n",
    "# Sort tags and remove duplicates\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "# Create training data\n",
    "X_train = [] # bag of words for each pattern\n",
    "y_train = [] # label for each tag\n",
    "\n",
    "for (pattern_sentence, tag) in xy:\n",
    "    # X: bag of words for each pattern\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "    # y: PyTorch CrossEntropyLoss\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label) # CrossEntropyLoss\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    # Dataset[idx]\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    # len(Dataset)\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "# hidden_size = 8\n",
    "# output_size = len(tags)\n",
    "# input_size = len(X_train[0])\n",
    "# learning_rate = 0.001\n",
    "# num_epochs = 1000\n",
    "\n",
    "dataset = ChatDataset()\n",
    "# Data loader which takes the dataset, shuffles it, and creates batches\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# # Neural network\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. PyTorch model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Save and load model and implement the chat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
